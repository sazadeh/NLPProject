{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = './'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, target_dir):\n",
    "    target_file = os.path.join(target_dir, os.path.basename(url))\n",
    "    urllib.request.urlretrieve(url, target_file)\n",
    "    assert os.path.isfile(target_file), \"File not succesfully downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\"https://raw.githubusercontent.com/jhlau/doc2vec/master/toy_data/train_docs.txt\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\"https://raw.githubusercontent.com/jhlau/doc2vec/master/toy_data/test_docs.txt\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will train a Paragraph Vectors / doc2vec model using gensim. You can find information on the gensim doc2vec api here: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "N.B. You should be using Python 3 for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data folder contains a train and test set with small sets of documents from the \"20 newsgroups\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do is the following:\n",
    "* Read a dataset with documents\n",
    "* Transform each document into a list of tokens (words)\n",
    "* Train a doc2vec model (DM)\n",
    "* Train a second model (DBOW)\n",
    "* Inspect the outcomes a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import doc2vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic settings\n",
    "HOMEDIR = './'\n",
    "CORPUS_FILE = os.path.join(HOMEDIR, \"data/train_docs.txt\")\n",
    "\n",
    "# file names for the models we'll be creating\n",
    "MODEL_FILE_DM = os.path.join(HOMEDIR, \"models/doc2vec_DM_v20210404.bin\")\n",
    "MODEL_FILE_DBOW = os.path.join(HOMEDIR, \"models/doc2vec_DBOW_v20210404.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sara/Google Drive/doc2vec-workshop-master'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the corpus. Each line is a document / paragraph. Optionally preprocess it first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flg_preprocess = False\n",
    "\n",
    "if flg_preprocess:\n",
    "    # quick & simple approach\n",
    "    docs = doc2vec.TaggedLineDocument(CORPUS_FILE)\n",
    "else:\n",
    "    # with pre-processing\n",
    "    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        docs = [simple_preprocess(line, deacc=False, min_len=1) for line in lines]\n",
    "        docs = [doc2vec.TaggedDocument(doc, tags=[i]) for i, doc in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['anarchism', 'is', 'a', 'political', 'philosophy', 'that', 'advocates', 'self', 'governed', 'societies', 'with', 'voluntary', 'institutions', 'these', 'are', 'often', 'described', 'as', 'stateless', 'societies', 'but', 'several', 'authors', 'have', 'defined', 'them', 'more', 'specifically', 'as', 'institutions', 'based', 'on', 'non', 'hierarchical', 'free', 'associations', 'anarchism', 'holds', 'the', 'state', 'to', 'be', 'undesirable', 'unnecessary', 'or', 'harmful', 'while', 'anti', 'statism', 'is', 'central', 'anarchism', 'entails', 'opposing', 'authority', 'or', 'hierarchical', 'organisation', 'in', 'the', 'conduct', 'of', 'human', 'relations', 'including', 'but', 'not', 'limited', 'to', 'the', 'state', 'system'], tags=[0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the data\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DM (Distributed Memory) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DM model\n",
    "model_dm = doc2vec.Doc2Vec(docs, \n",
    "                          # size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                           window=10, # window size for word context, on each side\n",
    "                           min_count=1, # minimum nr. of occurrences of a word\n",
    "                           sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                           workers=4, # for multicore processing\n",
    "                           hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                           dm=1, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                           negative=5, # how many words to use for negative sampling\n",
    "                           dbow_words=1, # train word vectors\n",
    "                           dm_concat=1, # concatenate vectors or sum/average them?\n",
    "                        #   iter=100 # nr of epochs to train\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later use\n",
    "model_dm.save(MODEL_FILE_DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DBOW (Distributed Bag Of Words) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# train DBOW model\n",
    "model_dbow = doc2vec.Doc2Vec(docs, \n",
    "                          #  size=200, # vector size, should be the same size as pre-trained embedding size when not using dm_concat\n",
    "                            window=10, # window size for word context, on each side\n",
    "                            min_count=1, # minimum nr. of occurrences of a word\n",
    "                            sample=1e-5, # threshold for undersampling high-frequency words\n",
    "                            workers=4, # for multicore processing\n",
    "                            hs=0, # if 1, use hierarchical softmax; if 0, use negative sampling\n",
    "                            dm=0, # if 1 use PV-DM, if 0 use PM-DBOW\n",
    "                            negative=5, # how many words to use for negative sampling\n",
    "                            dbow_words=1, # train word vectors\n",
    "                       #     iter=100 # nr of epochs to train\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save this one\n",
    "model_dbow.save(MODEL_FILE_DBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: Look at the model files that are now created in the models directory. Can you explain why there are 2 files for the DM model, but only 1 for the DBOW model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_similar(model, docs, ref_doc_id):\n",
    "    \"\"\"\n",
    "    For a given document, display the most similar ones in the corpus\n",
    "    \"\"\"\n",
    "    def print_doc(doc_id):\n",
    "        doc_txt = ' '.join(docs[doc_id].words)\n",
    "        print(\"[Doc {}]: {}\".format(doc_id, doc_txt))\n",
    "        \n",
    "    print(\"[Original document]\")\n",
    "    print_doc(ref_doc_id)\n",
    "    print(\"\\n[Most similar documents]\")\n",
    "    for doc_id, similarity in model.docvecs.most_similar(ref_doc_id, topn=3):\n",
    "        print(\"-----------------\")\n",
    "        print(\"similarity: {}\".format(similarity))\n",
    "        print_doc(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original document]\n",
      "[Doc 200]: single scattering albedo is used to define scattering of electromagnetic waves on small particles it depends on properties of the material lrb refractive index rrb the size of the particle or particles and the wavelength of the incoming radiation\n",
      "\n",
      "[Most similar documents]\n",
      "-----------------\n",
      "similarity: 0.9979395270347595\n",
      "[Doc 250]: a company of cavalry soldiers from huntsville alabama joined nathan bedford forrest s battalion in hopkinsville kentucky the company wore new uniforms with yellow trim on the sleeves collar and coat tails this led to them being greeted with yellowhammer and the name later was applied to all alabama troops in the confederate army\n",
      "-----------------\n",
      "similarity: 0.9979302883148193\n",
      "[Doc 335]: many commercial technology companies are headquartered in huntsville such as the network access company adtran computer graphics company intergraph design and manufacturer of it infrastructure avocent and provider deltacom cinram manufactures and distributes th century fox dvds and blu ray discs out of their huntsville plant\n",
      "-----------------\n",
      "similarity: 0.9979032278060913\n",
      "[Doc 950]: the fifth nanak guru arjun dev sacrificed his life to uphold carats of pure truth the greatest gift to humanity the guru granth the ninth guru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3700797b5c95>:12: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  for doc_id, similarity in model.docvecs.most_similar(ref_doc_id, topn=3):\n"
     ]
    }
   ],
   "source": [
    "show_most_similar(model_dbow, list(docs), 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = os.path.join(HOMEDIR, \"data/test_docs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data: each line into a list of tokens\n",
    "with open(test_data_file, \"r\") as f:\n",
    "    test_docs = [ x.strip().split() for x in f.readlines() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings for the test documents. Remember: this is an inference step that actually trains a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvecs = [model_dm.infer_vector(d, alpha=start_alpha, steps=infer_epoch) for d in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6078885e-03,  2.4099608e-03, -1.5234356e-03, -3.7185373e-03,\n",
       "        3.6129707e-03, -2.1931138e-03, -3.5841845e-03, -2.5726026e-03,\n",
       "        2.8449746e-03, -3.8756663e-03,  1.0904120e-03, -7.3614280e-04,\n",
       "        2.6318184e-03, -1.9826654e-04,  3.0851443e-04, -4.2312625e-03,\n",
       "        2.0832699e-03,  3.5922434e-03,  5.5884646e-04, -3.6848725e-03,\n",
       "       -4.5044329e-03,  4.6895659e-03, -2.0833074e-03,  1.4661247e-03,\n",
       "       -2.6929993e-03,  4.3324786e-03, -3.1219765e-03,  1.4456222e-03,\n",
       "        2.0738251e-03,  1.2590468e-03,  1.5912824e-03, -3.4159571e-03,\n",
       "        3.1452572e-03, -1.7045770e-03,  2.6687214e-03,  2.7725513e-03,\n",
       "        4.4544777e-03, -1.6557779e-04, -2.5399209e-03,  2.8612791e-03,\n",
       "       -1.9347386e-03, -4.5050815e-04,  3.1429667e-03,  1.3073338e-03,\n",
       "       -1.3034290e-04, -6.8750128e-04, -2.5468534e-03, -4.9226894e-03,\n",
       "        4.6723452e-03, -4.7310749e-03, -1.7769994e-03,  3.2929475e-03,\n",
       "       -3.8980865e-03,  4.3779421e-03, -2.9601860e-03, -3.4347062e-03,\n",
       "        2.2820590e-04, -1.4519314e-03, -7.2650128e-04, -1.9963486e-03,\n",
       "       -4.1334480e-03,  3.8757019e-03,  3.1017379e-03, -4.4887257e-04,\n",
       "        1.1927215e-04, -3.4952050e-04,  1.5570673e-04, -1.4842283e-03,\n",
       "        2.8704593e-04, -4.1773757e-03, -3.1947424e-03, -3.5191539e-03,\n",
       "       -4.8380392e-03, -3.9842154e-04,  4.7589597e-06, -2.7585416e-03,\n",
       "        1.8834113e-03,  1.8804815e-03, -4.3547750e-03, -4.0833629e-03,\n",
       "        3.7459573e-03, -6.0706778e-04, -1.0069204e-03, -4.1091442e-03,\n",
       "       -3.0571453e-03, -1.6635576e-03, -2.7447888e-03,  2.1375371e-03,\n",
       "       -3.4165902e-03, -2.1460715e-03, -1.5044395e-03, -3.8633123e-03,\n",
       "       -1.8118606e-04, -3.1843488e-03, -2.4728421e-03, -3.9106272e-03,\n",
       "       -3.0260116e-03, -2.4254788e-03, -4.4756178e-03,  3.1195898e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what one document embedding looks like\n",
    "test_docvecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a doc2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do in this exercise:\n",
    "* load a pre-trained doc2vec model\n",
    "* use it to infer document embeddings for our test set\n",
    "* cluster the documents based on the embeddings cosine distances\n",
    "* use t-SNE to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import doc2vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.cluster import kmeans\n",
    "from nltk.cluster import util\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic settings\n",
    "HOMEDIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FILE = os.path.join(HOMEDIR, \"data/train_docs.txt\")\n",
    "MODEL_FILE_DM = os.path.join(HOMEDIR, \"models/doc2vec_DM_v20210404.bin\")\n",
    "MODEL_FILE_DBOW = os.path.join(HOMEDIR, \"models/doc2vec_DBOW_v20210404.bin\")\n",
    "\n",
    "NUM_CLUSTERS = 20  # yes, you can change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus file and parse into token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    docs = [simple_preprocess(line, deacc=False, min_len=1) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read existing model and use it to derive document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = doc2vec.Doc2Vec.load(MODEL_FILE_DM)  # DM model chosen by default\n",
    "#model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)  # only keep what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine DM and DBOW models\n",
    "The authors of the paper suggest that combining the DM and the DBOW model works better than any single one. Do this by concatenating (you could also try to averaging or summing) the embeddings from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = doc2vec.Doc2Vec.load(MODEL_FILE_DM)\n",
    "model_dbow = doc2vec.Doc2Vec.load(MODEL_FILE_DBOW)\n",
    "\n",
    "docvecs_dm = [model_dm.infer_vector(d, alpha=0.01, steps=1000) for d in docs]\n",
    "docvecs_dbow = [model_dbow.infer_vector(d, alpha=0.01, steps=1000) for d in docs]\n",
    "\n",
    "docvecs = [docvecs_dm[i] + docvecs_dbow[i] for i, d in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer document vectors\n",
    "docvecs = [model.infer_vector(d, alpha=0.01, steps=1000) for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have document vectors, start clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = kmeans.KMeansClusterer(NUM_CLUSTERS, distance=util.cosine_distance, repeats=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = clusterer.cluster(docvecs, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({17: 52,\n",
       "         12: 43,\n",
       "         2: 41,\n",
       "         16: 90,\n",
       "         13: 47,\n",
       "         7: 55,\n",
       "         6: 57,\n",
       "         8: 49,\n",
       "         1: 39,\n",
       "         5: 40,\n",
       "         15: 51,\n",
       "         18: 46,\n",
       "         11: 64,\n",
       "         14: 41,\n",
       "         0: 51,\n",
       "         10: 53,\n",
       "         9: 50,\n",
       "         19: 43,\n",
       "         3: 49,\n",
       "         4: 39})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many documents per cluster?\n",
    "collections.Counter(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_in_cluster(cluster_idx):\n",
    "    return [doc for i, doc in enumerate(docs) if cluster_assignments[i] == cluster_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(doc_vec, topic_vecs):\n",
    "    \"\"\"\n",
    "    For a given document, give the topic distribution (softmax probabilities for all topics)\n",
    "    \"\"\"\n",
    "    similarities = [np.dot(doc_vec, topic_vec) for topic_vec in topic_vecs]\n",
    "    return np.exp(similarities) / np.sum(np.exp(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the topics as the cluster centroids. Then find the nearest-neighbor words to describe the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vecs = clusterer.means()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize topics using t-SNE\n",
    "What we're going to do now:\n",
    "* reduce 100-dim vector space to 2 dimensions\n",
    "* plot all documents in this 2D space\n",
    "* use color to show the clustering\n",
    "* inspect how close / afar certain documents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import push_notebook, output_notebook, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tsne = TSNE(n_components=2, perplexity=30, init='pca').fit_transform(docvecs)\n",
    "docs_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix with topic proportion per doc per topic\n",
    "doc_topic_matrix = [get_document_topics(docvec, topic_vecs) for docvec in docvecs]\n",
    "# select highest topic prob\n",
    "prob_max_topic = np.max(doc_topic_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedata = {\n",
    "    'x': docs_tsne[:, 0],\n",
    "    'y': docs_tsne[:, 1],\n",
    "    'color': colormap[cluster_assignments],\n",
    "    'alpha': prob_max_topic * 50,\n",
    "    'content': lines,\n",
    "    'topic_key': cluster_assignments\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make and show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot = bp.figure(plot_width=1600, plot_height=900,\n",
    "                      title=\"Topics\",\n",
    "                      tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n",
    "                      x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "tsne_plot.scatter(x='x', \n",
    "                  y='y',\n",
    "                  color='color',\n",
    "                  size='alpha',\n",
    "                  #size=10,\n",
    "                  source=bp.ColumnDataSource(sourcedata)\n",
    "                 )\n",
    "\n",
    "# add hover tooltips\n",
    "hover = tsne_plot.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
